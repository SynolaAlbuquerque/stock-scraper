with open('/tesla_headlines_nicely_formatted.txt', 'r', encoding='utf-8') as f:
    aditya_headlines_string = f.read()

!pip install -q yfinance vaderSentiment textblob scikit-learn matplotlib pandas numpy ta

import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib.pyplot as plt
from datetime import datetime
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

print("âœ“ All libraries imported successfully!\n")

# ============================================================================
# STEP 2: YOUR HEADLINES DATA - PASTE YOUR HEADLINES HERE
# ============================================================================

headlines_text = aditya_headlines_string

# ============================================================================
# STEP 3: Parse Headlines
# ============================================================================
print("="*80)
print("PARSING HEADLINES")
print("="*80)

def parse_headlines(text):
    """Parse headlines from the given text format"""
    lines = text.strip().split('\n')
    headlines = []
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        if line and line[0].isdigit() and '.' in line:
            headline = line.split('.', 1)[1].strip()
            
            if i + 1 < len(lines) and 'Source:' in lines[i + 1]:
                source = lines[i + 1].replace('Source:', '').strip()
            else:
                source = 'Unknown'
            
            if i + 2 < len(lines) and 'Published on:' in lines[i + 2]:
                date_str = lines[i + 2].replace('Published on:', '').strip()
                try:
                    date = datetime.strptime(date_str, '%d %b %Y, %I:%M %p')
                except:
                    try:
                        date = datetime.strptime(date_str, '%d %b %Y')
                    except:
                        date = None
            else:
                date = None
            
            if date:
                headlines.append({
                    'headline': headline,
                    'source': source,
                    'date': date,
                    'date_only': date.date()
                })
            
            i += 3
        else:
            i += 1
    
    return pd.DataFrame(headlines)

df_headlines = parse_headlines(headlines_text)
print(f"âœ“ Parsed {len(df_headlines)} headlines")
print(f"âœ“ Date range: {df_headlines['date_only'].min()} to {df_headlines['date_only'].max()}\n")

# ============================================================================
# STEP 4: Sentiment Analysis
# ============================================================================
print("="*80)
print("SENTIMENT ANALYSIS")
print("="*80)

vader = SentimentIntensityAnalyzer()

def get_vader_sentiment(text):
    return vader.polarity_scores(text)['compound']

def get_textblob_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity, blob.sentiment.subjectivity

df_headlines['vader_compound'] = df_headlines['headline'].apply(get_vader_sentiment)
df_headlines[['textblob_polarity', 'textblob_subjectivity']] = df_headlines['headline'].apply(
    lambda x: pd.Series(get_textblob_sentiment(x))
)

print(f"âœ“ Sentiment analysis complete!")
print(f"âœ“ VADER range: {df_headlines['vader_compound'].min():.3f} to {df_headlines['vader_compound'].max():.3f}")
print(f"âœ“ TextBlob range: {df_headlines['textblob_polarity'].min():.3f} to {df_headlines['textblob_polarity'].max():.3f}\n")

# ============================================================================
# STEP 5: Daily Aggregation
# ============================================================================
print("="*80)
print("DAILY AGGREGATION")
print("="*80)

df_daily_sentiment = df_headlines.groupby('date_only').agg({
    'vader_compound': 'mean',
    'textblob_polarity': 'mean',
    'textblob_subjectivity': 'mean',
    'headline': 'count'
}).rename(columns={'headline': 'headline_count'}).reset_index()

print(f"âœ“ Aggregated sentiment for {len(df_daily_sentiment)} unique days\n")

# ============================================================================
# STEP 6: Download Stock Data
# ============================================================================
print("="*80)
print("DOWNLOADING STOCK DATA")
print("="*80)

# Download more historical data for better technical indicators
df_stock = yf.download("TSLA", start="2021-01-05", end="2025-10-26", interval="1d", progress=False)

if isinstance(df_stock.columns, pd.MultiIndex):
    df_stock.columns = df_stock.columns.get_level_values(0)

df_stock = df_stock.reset_index()
df_stock['date_only'] = pd.to_datetime(df_stock['Date']).dt.date

print(f"âœ“ Downloaded {len(df_stock)} days of stock data")
print(f"âœ“ Price range: ${df_stock['Close'].min():.2f} to ${df_stock['Close'].max():.2f}\n")

# ============================================================================
# STEP 7: CALCULATE TECHNICAL INDICATORS
# ============================================================================
print("="*80)
print("CALCULATING TECHNICAL INDICATORS")
print("="*80)

# 1. RSI (Relative Strength Index)
def calculate_rsi(data, window=14):
    delta = data.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

df_stock['rsi_14'] = calculate_rsi(df_stock['Close'], window=14)
print("âœ“ RSI calculated")

# 2. MACD (Moving Average Convergence Divergence)
def calculate_macd(data, fast=12, slow=26, signal=9):
    ema_fast = data.ewm(span=fast, adjust=False).mean()
    ema_slow = data.ewm(span=slow, adjust=False).mean()
    macd = ema_fast - ema_slow
    signal_line = macd.ewm(span=signal, adjust=False).mean()
    macd_histogram = macd - signal_line
    return macd, signal_line, macd_histogram

df_stock['macd'], df_stock['macd_signal'], df_stock['macd_histogram'] = calculate_macd(df_stock['Close'])
print("âœ“ MACD calculated")

# 3. Bollinger Bands
def calculate_bollinger_bands(data, window=20, num_std=2):
    rolling_mean = data.rolling(window=window).mean()
    rolling_std = data.rolling(window=window).std()
    upper_band = rolling_mean + (rolling_std * num_std)
    lower_band = rolling_mean - (rolling_std * num_std)
    bb_width = upper_band - lower_band
    bb_position = (data - lower_band) / (upper_band - lower_band)
    return upper_band, lower_band, bb_width, bb_position

df_stock['bb_upper'], df_stock['bb_lower'], df_stock['bb_width'], df_stock['bb_position'] = calculate_bollinger_bands(df_stock['Close'])
print("âœ“ Bollinger Bands calculated")

# 4. Moving Averages (EMA and SMA)
df_stock['ema_5'] = df_stock['Close'].ewm(span=5, adjust=False).mean()
df_stock['ema_10'] = df_stock['Close'].ewm(span=10, adjust=False).mean()
df_stock['ema_20'] = df_stock['Close'].ewm(span=20, adjust=False).mean()
df_stock['sma_5'] = df_stock['Close'].rolling(window=5).mean()
df_stock['sma_10'] = df_stock['Close'].rolling(window=10).mean()
df_stock['sma_20'] = df_stock['Close'].rolling(window=20).mean()
print("âœ“ Moving Averages calculated")

# 5. Price relative to moving averages
df_stock['price_to_sma5'] = df_stock['Close'] / df_stock['sma_5']
df_stock['price_to_sma10'] = df_stock['Close'] / df_stock['sma_10']
df_stock['price_to_sma20'] = df_stock['Close'] / df_stock['sma_20']
print("âœ“ Price ratios calculated")

# 6. Momentum indicators
df_stock['momentum_5'] = df_stock['Close'] - df_stock['Close'].shift(5)
df_stock['momentum_10'] = df_stock['Close'] - df_stock['Close'].shift(10)
df_stock['roc_5'] = df_stock['Close'].pct_change(5) * 100  # Rate of Change
df_stock['roc_10'] = df_stock['Close'].pct_change(10) * 100
print("âœ“ Momentum indicators calculated")

# 7. Volume indicators
df_stock['volume_sma_5'] = df_stock['Volume'].rolling(window=5).mean()
df_stock['volume_sma_10'] = df_stock['Volume'].rolling(window=10).mean()
df_stock['volume_ratio'] = df_stock['Volume'] / df_stock['volume_sma_5']
df_stock['volume_change'] = df_stock['Volume'].pct_change()
print("âœ“ Volume indicators calculated")

# 8. On-Balance Volume (OBV)
def calculate_obv(close, volume):
    obv = [0]
    for i in range(1, len(close)):
        if close.iloc[i] > close.iloc[i-1]:
            obv.append(obv[-1] + volume.iloc[i])
        elif close.iloc[i] < close.iloc[i-1]:
            obv.append(obv[-1] - volume.iloc[i])
        else:
            obv.append(obv[-1])
    return obv

df_stock['obv'] = calculate_obv(df_stock['Close'], df_stock['Volume'])
df_stock['obv_ema'] = pd.Series(df_stock['obv']).ewm(span=10, adjust=False).mean()
print("âœ“ OBV calculated")

# 9. Volatility
df_stock['volatility_5'] = df_stock['Close'].rolling(window=5).std()
df_stock['volatility_10'] = df_stock['Close'].rolling(window=10).std()
df_stock['atr'] = df_stock['High'] - df_stock['Low']  # Simplified ATR
df_stock['atr_sma'] = df_stock['atr'].rolling(window=14).mean()
print("âœ“ Volatility indicators calculated")

# 10. Price changes
df_stock['pct_change_1d'] = df_stock['Close'].pct_change(1)
df_stock['pct_change_5d'] = df_stock['Close'].pct_change(5)
df_stock['pct_change_10d'] = df_stock['Close'].pct_change(10)
print("âœ“ Price change features calculated")

print(f"\nâœ“ Total technical indicators created: 40+\n")

# ============================================================================
# STEP 8: Merge Data
# ============================================================================
print("="*80)
print("MERGING DATA")
print("="*80)

# Filter stock data to match headline date range
df_stock_filtered = df_stock[df_stock['date_only'] >= df_headlines['date_only'].min()].copy()

df_merged = pd.merge(df_stock_filtered, df_daily_sentiment, on='date_only', how='left')

sentiment_cols = ['vader_compound', 'textblob_polarity', 'textblob_subjectivity', 'headline_count']
df_merged[sentiment_cols] = df_merged[sentiment_cols].fillna(0)

print(f"âœ“ Merged dataset: {len(df_merged)} rows\n")

# ============================================================================
# STEP 9: Feature Engineering (Sentiment + Lagged)
# ============================================================================
print("="*80)
print("FEATURE ENGINEERING")
print("="*80)

df_merged = df_merged.sort_values('date_only').reset_index(drop=True)

# Sentiment rolling features
df_merged['vader_rolling_3d'] = df_merged['vader_compound'].rolling(window=3, min_periods=1).mean()
df_merged['vader_rolling_5d'] = df_merged['vader_compound'].rolling(window=5, min_periods=1).mean()
df_merged['textblob_rolling_3d'] = df_merged['textblob_polarity'].rolling(window=3, min_periods=1).mean()
df_merged['textblob_rolling_5d'] = df_merged['textblob_polarity'].rolling(window=5, min_periods=1).mean()

# Sentiment momentum
df_merged['sentiment_momentum'] = df_merged['vader_compound'].diff()
df_merged['sentiment_volatility'] = df_merged['vader_compound'].rolling(window=5).std()

# Lagged stock price features
df_merged['close_lag_1d'] = df_merged['Close'].shift(1)
df_merged['close_lag_3d'] = df_merged['Close'].shift(3)
df_merged['close_lag_5d'] = df_merged['Close'].shift(5)

print(f"âœ“ Feature engineering complete!")
print(f"Rows before dropping NaNs: {len(df_merged)}")
df_merged = df_merged.dropna()
print(f"Rows after dropping NaNs: {len(df_merged)}\n")

# ============================================================================
# STEP 10: Prepare Features and Target
# ============================================================================
print("="*80)
print("PREPARING FEATURES AND TARGET")
print("="*80)

# Select best features (Technical + Sentiment)
feature_cols = [
    # Technical Indicators
    'rsi_14', 'macd', 'macd_signal', 'macd_histogram',
    'bb_width', 'bb_position',
    'ema_5', 'ema_10', 'ema_20',
    'price_to_sma5', 'price_to_sma10', 'price_to_sma20',
    'momentum_5', 'momentum_10', 'roc_5', 'roc_10',
    'volume_ratio', 'volume_change',
    'obv', 'obv_ema',
    'volatility_5', 'volatility_10', 'atr_sma',
    'pct_change_1d', 'pct_change_5d', 'pct_change_10d',
    # Lagged prices
    'close_lag_1d', 'close_lag_3d', 'close_lag_5d',
    # Sentiment features
    'vader_compound', 'textblob_polarity', 'textblob_subjectivity',
    'vader_rolling_3d', 'vader_rolling_5d',
    'textblob_rolling_3d', 'textblob_rolling_5d',
    'sentiment_momentum', 'sentiment_volatility',
    'headline_count'
]

X = df_merged[feature_cols]
y = df_merged['Close']

print(f"âœ“ Features shape: {X.shape}")
print(f"âœ“ Target shape: {y.shape}")
print(f"âœ“ Total features: {len(feature_cols)}\n")

# ============================================================================
# STEP 11: Feature Scaling (Important for gradient boosting!)
# ============================================================================
print("="*80)
print("FEATURE SCALING")
print("="*80)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)

print(f"âœ“ Features scaled using StandardScaler\n")

# ============================================================================
# STEP 12: Train-Test Split
# ============================================================================
print("="*80)
print("TRAIN-TEST SPLIT (Chronological 80-20)")
print("="*80)

split_idx = int(len(df_merged) * 0.8)

X_train = X_scaled.iloc[:split_idx]
X_test = X_scaled.iloc[split_idx:]
y_train = y.iloc[:split_idx]
y_test = y.iloc[split_idx:]

print(f"âœ“ Training set: {len(X_train)} samples")
print(f"âœ“ Test set: {len(X_test)} samples\n")

# ============================================================================
# STEP 13: Train GRADIENT BOOSTING Model (Better than Random Forest!)
# ============================================================================
print("="*80)
print("TRAINING GRADIENT BOOSTING MODEL")
print("="*80)

gb_model = GradientBoostingRegressor(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=5,
    min_samples_split=4,
    min_samples_leaf=2,
    subsample=0.8,
    random_state=42,
    verbose=0
)

gb_model.fit(X_train, y_train)
print(f"âœ“ Gradient Boosting model training complete!\n")

# Also train Random Forest for comparison
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    min_samples_split=4,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)
print(f"âœ“ Random Forest model training complete!\n")

# ============================================================================
# STEP 14: Make Predictions and Evaluate
# ============================================================================
print("="*80)
print("MODEL EVALUATION")
print("="*80)

# Gradient Boosting predictions
y_train_pred_gb = gb_model.predict(X_train)
y_test_pred_gb = gb_model.predict(X_test)

train_mae_gb = mean_absolute_error(y_train, y_train_pred_gb)
train_rmse_gb = np.sqrt(mean_squared_error(y_train, y_train_pred_gb))
test_mae_gb = mean_absolute_error(y_test, y_test_pred_gb)
test_rmse_gb = np.sqrt(mean_squared_error(y_test, y_test_pred_gb))
test_r2_gb = r2_score(y_test, y_test_pred_gb)

# Random Forest predictions
y_train_pred_rf = rf_model.predict(X_train)
y_test_pred_rf = rf_model.predict(X_test)

train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)
train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)
test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
test_r2_rf = r2_score(y_test, y_test_pred_rf)

print("\n" + "="*80)
print("PERFORMANCE COMPARISON")
print("="*80)

print("\nðŸ“Š GRADIENT BOOSTING (Primary Model):")
print(f"  Training   - MAE: ${train_mae_gb:.2f}, RMSE: ${train_rmse_gb:.2f}")
print(f"  Test       - MAE: ${test_mae_gb:.2f}, RMSE: ${test_rmse_gb:.2f}, RÂ²: {test_r2_gb:.4f}")

print("\nðŸ“Š RANDOM FOREST (Comparison):")
print(f"  Training   - MAE: ${train_mae_rf:.2f}, RMSE: ${train_rmse_rf:.2f}")
print(f"  Test       - MAE: ${test_mae_rf:.2f}, RMSE: ${test_rmse_rf:.2f}, RÂ²: {test_r2_rf:.4f}")

# Choose best model
if test_mae_gb < test_mae_rf:
    print("\nâœ… Gradient Boosting performs better! Using GB predictions.")
    y_train_pred = y_train_pred_gb
    y_test_pred = y_test_pred_gb
    test_mae = test_mae_gb
    test_rmse = test_rmse_gb
    best_model = gb_model
    model_name = "Gradient Boosting"
else:
    print("\nâœ… Random Forest performs better! Using RF predictions.")
    y_train_pred = y_train_pred_rf
    y_test_pred = y_test_pred_rf
    test_mae = test_mae_rf
    test_rmse = test_rmse_rf
    best_model = rf_model
    model_name = "Random Forest"

# Feature importance
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nðŸ” Top 10 Most Important Features ({model_name}):")
print(feature_importance.head(15).to_string(index=False))
print()

# ============================================================================
# STEP 15: ENHANCED Visualization
# ============================================================================
print("="*80)
print("CREATING VISUALIZATIONS")
print("="*80)

fig, axes = plt.subplots(3, 2, figsize=(16, 14))

dates = df_merged['date_only'].values

# Plot 1: Full timeline
ax1 = axes[0, 0]
ax1.plot(dates, y.values, 'o-', label='Actual Price', color='blue', linewidth=2, markersize=6)
ax1.plot(dates[:split_idx], y_train_pred, 's-', label='Predicted (Train)', color='green', linewidth=1.5, markersize=4, alpha=0.7)
ax1.plot(dates[split_idx:], y_test_pred, '^-', label='Predicted (Test)', color='red', linewidth=2, markersize=6)
ax1.axvline(x=dates[split_idx], color='black', linestyle='--', linewidth=2, label='Train-Test Split')
ax1.set_xlabel('Date', fontsize=11)
ax1.set_ylabel('Stock Price ($)', fontsize=11)
ax1.set_title(f'Tesla Stock: Actual vs Predicted ({model_name})', fontsize=13, fontweight='bold')
ax1.legend(loc='best', fontsize=9)
ax1.grid(True, alpha=0.3)
ax1.tick_params(axis='x', rotation=45)

# Plot 2: Test set zoom
ax2 = axes[0, 1]
test_dates = dates[split_idx:]
ax2.plot(test_dates, y_test.values, 'o-', label='Actual Price', color='blue', linewidth=2.5, markersize=8)
ax2.plot(test_dates, y_test_pred, '^-', label='Predicted Price', color='red', linewidth=2.5, markersize=8)
ax2.set_xlabel('Date', fontsize=11)
ax2.set_ylabel('Stock Price ($)', fontsize=11)
ax2.set_title(f'Test Set (MAE: ${test_mae:.2f}, RMSE: ${test_rmse:.2f}, RÂ²: {test_r2_gb:.3f})', fontsize=12, fontweight='bold')
ax2.legend(loc='best', fontsize=10)
ax2.grid(True, alpha=0.3)
ax2.tick_params(axis='x', rotation=45)

# Plot 3: Prediction errors
ax3 = axes[1, 0]
ax3.scatter(y_test, y_test_pred, alpha=0.6, s=100, edgecolors='black')
ax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')
ax3.set_xlabel('Actual Price ($)', fontsize=11)
ax3.set_ylabel('Predicted Price ($)', fontsize=11)
ax3.set_title('Actual vs Predicted (Test Set)', fontsize=12, fontweight='bold')
ax3.legend(fontsize=10)
ax3.grid(True, alpha=0.3)

# Plot 4: Residuals
ax4 = axes[1, 1]
residuals = y_test.values - y_test_pred
ax4.scatter(y_test_pred, residuals, alpha=0.6, s=100, edgecolors='black')
ax4.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax4.set_xlabel('Predicted Price ($)', fontsize=11)
ax4.set_ylabel('Residuals ($)', fontsize=11)
ax4.set_title('Residual Plot', fontsize=12, fontweight='bold')
ax4.grid(True, alpha=0.3)

# Plot 5: Feature importance
ax5 = axes[2, 0]
top_features = feature_importance.head(12)
colors = ['red' if 'sentiment' in f or 'vader' in f or 'textblob' in f else 'steelblue' 
          for f in top_features['feature']]
ax5.barh(top_features['feature'], top_features['importance'], color=colors, edgecolor='black')
ax5.set_xlabel('Importance', fontsize=11)
ax5.set_title('Top 12 Feature Importance (Red = Sentiment)', fontsize=12, fontweight='bold')
ax5.grid(axis='x', alpha=0.3)

# Plot 6: Error distribution
ax6 = axes[2, 1]
ax6.hist(residuals, bins=15, edgecolor='black', color='skyblue', alpha=0.7)
ax6.axvline(x=0, color='r', linestyle='--', linewidth=2)
ax6.set_xlabel('Prediction Error ($)', fontsize=11)
ax6.set_ylabel('Frequency', fontsize=11)
ax6.set_title(f'Error Distribution (Mean: ${residuals.mean():.2f})', fontsize=12, fontweight='bold')
ax6.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('tesla_enhanced_prediction.png', dpi=300, bbox_inches='tight')
print("âœ“ Enhanced visualization saved as 'tesla_enhanced_prediction.png'\n")
plt.show()

# ============================================================================
# STEP 16: Save Results
# ============================================================================
print("="*80)
print("SAVING RESULTS")
print("="*80)

results_df = df_merged.copy()
results_df['predicted_close'] = np.concatenate([y_train_pred, y_test_pred])
results_df['prediction_error'] = results_df['Close'] - results_df['predicted_close']
results_df['absolute_error'] = np.abs(results_df['prediction_error'])
results_df['percentage_error'] = (results_df['absolute_error'] / results_df['Close']) * 100
results_df['set'] = ['train'] * split_idx + ['test'] * (len(results_df) - split_idx)

results_df.to_csv('tesla_enhanced_predictions.csv', index=False)
feature_importance.to_csv('feature_importance.csv', index=False)

print("âœ“ Results saved to 'tesla_enhanced_predictions.csv'")
print("âœ“ Feature importance saved to 'feature_importance.csv'\n")

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("="*80)
print("PROJECT COMPLETE! ðŸŽ‰")
print("="*80)

print(f"\nðŸ“ˆ IMPROVEMENTS OVER BASELINE:")
print(f"   â€¢ Added 40+ technical indicators")
print(f"   â€¢ Used {model_name} instead of basic Random Forest")
print(f"   â€¢ Feature scaling for better performance")
print(f"   â€¢ Enhanced sentiment features")
print(f"   â€¢ Total features: {len(feature_cols)} (vs 12 in baseline)")

print(f"\nðŸ“Š FINAL TEST PERFORMANCE:")
print(f"   â€¢ MAE:  ${test_mae:.2f}")
print(f"   â€¢ RMSE: ${test_rmse:.2f}")
print(f"   â€¢ RÂ² Score: {test_r2_gb:.4f}")
print(f"   â€¢ Mean Error: ${residuals.mean():.2f}")
print(f"   â€¢ Mean % Error: {results_df[results_df['set']=='test']['percentage_error'].mean():.2f}%")

print(f"\nðŸŽ¯ KEY INSIGHTS:")
technical_importance = feature_importance[~feature_importance['feature'].str.contains('sentiment|vader|textblob|headline')]['importance'].sum()
sentiment_importance = feature_importance[feature_importance['feature'].str.contains('sentiment|vader|textblob|headline')]['importance'].sum()
print(f"   â€¢ Technical indicators contribute: {technical_importance*100:.1f}% of predictive power")
print(f"   â€¢ Sentiment features contribute: {sentiment_importance*100:.1f}% of predictive power")
print(f"   â€¢ Most important feature: {feature_importance.iloc[0]['feature']}")

print("\nâœ… Files Generated:")
print("   1. tesla_enhanced_prediction.png (6 comprehensive plots)")
print("   2. tesla_enhanced_predictions.csv (all predictions)")
print("   3. feature_importance.csv (feature rankings)")

print("\n" + "="*80)
print("Remember to paste your full headlines data in headlines_text variable!")
print("="*80)
